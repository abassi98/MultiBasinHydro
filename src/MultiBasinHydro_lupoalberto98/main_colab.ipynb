{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cca0db3-4ef7-49c4-a54e-dbdf5be12fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping \n",
    "from torchvision import transforms, datasets\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# user functions\n",
    "from dataset import CamelDataset\n",
    "from models import Hydro_LSTM_AE\n",
    "from utils import Scale_Data, MetricsCallback, NSELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e15da3-822a-4f5e-be67-baaa7c4fac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x146129130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "# set seed\n",
    "##########################################################\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad992be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# mount drive\n",
    "##########################################################\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b1367c-b72c-456b-9202-f023a5707db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Camel ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m camel_dataset \u001b[39m=\u001b[39m CamelDataset(dates, force_attributes, data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../../basin_dataset_public_v1p2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m#dataset.adjust_dates() # adjust dates if necessary\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m camel_dataset\u001b[39m.\u001b[39;49mload_data() \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m num_basins \u001b[39m=\u001b[39m camel_dataset\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m()\n\u001b[1;32m     12\u001b[0m seq_len \u001b[39m=\u001b[39m camel_dataset\u001b[39m.\u001b[39mseq_len\n",
      "File \u001b[0;32m~/Documents/PHD ZUrich/MultiBasinHydro/src/MultiBasinHydro_lupoalberto98/dataset.py:155\u001b[0m, in \u001b[0;36mCamelDataset.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m# read forcing data \u001b[39;00m\n\u001b[1;32m    154\u001b[0m df_forcing \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_forcing_data,delim_whitespace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skiprows\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m force_dates \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([datetime\u001b[39m.\u001b[39mdate(df_forcing\u001b[39m.\u001b[39miloc[i,\u001b[39m0\u001b[39m], df_forcing\u001b[39m.\u001b[39miloc[i,\u001b[39m1\u001b[39m], df_forcing\u001b[39m.\u001b[39miloc[i,\u001b[39m2\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df_forcing))])\n\u001b[1;32m    157\u001b[0m \u001b[39m# control length and assert they are equal\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m#assert len(flow_data) == len(df_forcing)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[39m# get rid of cathcments with false values and whose dates range is not the\u001b[39;00m\n\u001b[1;32m    161\u001b[0m interval_force_dates_bool \u001b[39m=\u001b[39m force_dates[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_date \u001b[39mand\u001b[39;00m force_dates[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend_date\n",
      "File \u001b[0;32m~/Documents/PHD ZUrich/MultiBasinHydro/src/MultiBasinHydro_lupoalberto98/dataset.py:155\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m# read forcing data \u001b[39;00m\n\u001b[1;32m    154\u001b[0m df_forcing \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_forcing_data,delim_whitespace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skiprows\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m force_dates \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([datetime\u001b[39m.\u001b[39mdate(df_forcing\u001b[39m.\u001b[39miloc[i,\u001b[39m0\u001b[39m], df_forcing\u001b[39m.\u001b[39;49miloc[i,\u001b[39m1\u001b[39;49m], df_forcing\u001b[39m.\u001b[39miloc[i,\u001b[39m2\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df_forcing))])\n\u001b[1;32m    157\u001b[0m \u001b[39m# control length and assert they are equal\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m#assert len(flow_data) == len(df_forcing)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[39m# get rid of cathcments with false values and whose dates range is not the\u001b[39;00m\n\u001b[1;32m    161\u001b[0m interval_force_dates_bool \u001b[39m=\u001b[39m force_dates[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_date \u001b[39mand\u001b[39;00m force_dates[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend_date\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexing.py:1066\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[1;32m   1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3912\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3893\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3894\u001b[0m \u001b[39mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[1;32m   3895\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3909\u001b[0m \u001b[39m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[1;32m   3910\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3911\u001b[0m \u001b[39mif\u001b[39;00m takeable:\n\u001b[0;32m-> 3912\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ixs(col, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m   3913\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m   3915\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3732\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3729\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_box_col_values(col_mgr, i)\n\u001b[1;32m   3731\u001b[0m \u001b[39m# this is a cached value, mark it so\u001b[39;00m\n\u001b[0;32m-> 3732\u001b[0m result\u001b[39m.\u001b[39;49m_set_as_cached(label, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m   3733\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:1272\u001b[0m, in \u001b[0;36mSeries._set_as_cached\u001b[0;34m(self, item, cacher)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_as_cached\u001b[39m(\u001b[39mself\u001b[39m, item, cacher) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[39m    Set the _cacher attribute on the calling object with a weakref to\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[39m    cacher.\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cacher \u001b[39m=\u001b[39m (item, weakref\u001b[39m.\u001b[39mref(cacher))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:5922\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5919\u001b[0m \u001b[39m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[1;32m   5920\u001b[0m \u001b[39m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[1;32m   5921\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set:\n\u001b[0;32m-> 5922\u001b[0m     \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, value)\n\u001b[1;32m   5923\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata:\n\u001b[1;32m   5924\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, name, value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########################################################  \n",
    "# dataset and dataloaders\n",
    "##########################################################\n",
    "# Dataset\n",
    "#dates = [\"1989/10/01\", \"2009/09/30\"] \n",
    "dates = [\"1980/10/01\", \"2010/09/30\"] # interval dates to pick\n",
    "force_attributes = [\"prcp(mm/day)\", \"srad(W/m2)\", \"tmin(C)\", \"tmax(C)\", \"vp(Pa)\"] # force attributes to use\n",
    "camel_dataset = CamelDataset(dates, force_attributes, data_path=\"gdrive/MyDrive/basin_dataset_public_v1p2\") # use dataset on google drive\n",
    "#dataset.adjust_dates() # adjust dates if necessary\n",
    "camel_dataset.load_data() # load data\n",
    "num_basins = camel_dataset.__len__()\n",
    "seq_len = camel_dataset.seq_len\n",
    "print(\"Number of basins: %d\" %num_basins)\n",
    "print(\"Sequence length: %d\" %seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce76123",
   "metadata": {},
   "outputs": [],
   "source": [
    " ### Set proper device and train\n",
    "# check cpus and gpus available\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(\"Num of cpus: %d\"%num_cpus)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Num of gpus: %d\"%num_gpus)\n",
    "    \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "\n",
    "### Dataloader\n",
    "batch_size = 32\n",
    "# split 80/10/10\n",
    "num_workers = 0\n",
    "print(\"Number of workers: %d\"%num_workers)\n",
    "\n",
    "num_train_data = int(num_basins * 0.8) \n",
    "num_val_data = num_basins - num_train_data\n",
    "#num_test_data = num_basins - num_train_data - num_val_data\n",
    "print(\"Train basins: %d\" %num_train_data)\n",
    "print(\"Validation basins: %d\" %num_val_data)\n",
    "#print(\"Test basins: %d\" %num_test_data)\n",
    "    \n",
    "train_dataset, val_dataset = random_split(camel_dataset, (num_train_data, num_val_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=num_val_data, num_workers=num_workers, shuffle=False)\n",
    "#test_dataloader = DataLoader(val_dataset, batch_size=num_test_data, num_workers=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc234762",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# define the model\n",
    "loss_fn = NSELoss()\n",
    "# possibly adjust kernel sizes according to seq_len\n",
    "model = Hydro_LSTM_AE(in_channels=(1,8,16), \n",
    "                    out_channels=(8,16,32), \n",
    "                    kernel_sizes=(6,3,5), \n",
    "                    encoded_space_dim=27,\n",
    "                    drop_p=0.5,\n",
    "                    seq_len=seq_len,\n",
    "                    lr = 1e-4,\n",
    "                    act=nn.LeakyReLU,\n",
    "                    loss_fn=loss_fn,\n",
    "                    lstm_hidden_units=256,\n",
    "                    layers_num=2,\n",
    "                    linear=512,\n",
    "                    num_force_attributes = len(force_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# training \n",
    "##########################################################\n",
    "\n",
    "    \n",
    "# define callbacks\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 10, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=100,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=\"checkpoints/lstm-ae/\",\n",
    "        filename=\"hydro-lstm-ae-{epoch:02d}\",\n",
    "    )\n",
    "\n",
    "    \n",
    "# define trainer \n",
    "trainer = pl.Trainer(max_epochs=3000, callbacks=[metrics_callback, checkpoint_callback], accelerator=str(device), check_val_every_n_epoch=10, logger=False)\n",
    "    \n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders = val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (v3.10.8:aaaf517424, Oct 11 2022, 10:14:40) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
